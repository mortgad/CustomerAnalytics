---
title: "The relationships between Customer Perceptions and the Likelihood of Future Business"
author: "Copyright: Ana Alina Tudoran"
date: "12/01/2022"
geometry: margin=0.9in  
fontsize: 11pt
output:
  html_document: default
  pdf_document: 
  latex_engine: xelatex
  word_document: default
---
&nbsp;
&nbsp;
&nbsp;
&nbsp;


##### 0. Problem & objective (we use the datafile from Hair et al., Multivariate Data Analysis, Pearson)
HBAT is a manufacturer of paper products who sells products to two market segments: the newsprint industry and the magazine industry. The current market is very competitive, so the manufacturer wants to understand how its customers perceive the company and make purchasing decisions, in order to enforce customers loyalty. The manufacturer commissioned a study asking its customers to complete a questionnaire on a secure website. In total, 100 customers - purchasing managers from different firms - buying from HBAT completed the questionnaire. The data consist of three main pieces of information:
&nbsp;
&nbsp;

•	A 1st type of information is available from HBAT ́s data warehouse and includes information on:
-	customer type in terms of length of purchase relationship (X1)
-	industry type(X2)
-	size of the customer(X3)
-	region of the customer(X4) 
-	distribution system(X5)

•	The 2nd type of information is collected based on the online questionnaire and includes consumers’ perceptions of HBAT ́s performance on 13 attributes using a continuous 0-10 (line) scale with 10 being “Excellent” and 0 being “Poor”. The 13 attributes are:
-	X6 Product quality
-	X7 E-commerce
-	X8 Technical support
-	X9 Complaint resolution
-	X10 Advertising
-	X11 Product line
-	X12 Salesforce image
-	X13 Competitive pricing
-	X14 Warranty and claims
-	X15 Packaging
-	X16 Order and billing
-	X17 Price flexibility
-	X18 Delivery speed

• The 3rd type of information relates to purchase outcomes and business relationships:
-	satisfaction with HBAT, future purchase intention etc.  (X19-X22)
-	whether the firm would consider a strategic alliance/partnership with HBAT (X23).

&nbsp;
&nbsp;

The dataset (HBAT.sav) consists of data for n = 100 customers. Each observation contains information on 23 variables described above. Consistent with the marketing theory, there is an underlying factor structure in the data. When designing the study, the company has clearly 4 types of factors in their mind. They expect that the customer satisfaction is determined by the following four type of perceptions: perceptions about the product value, perceptions about the marketing actions, perceptions about the customer service and perceptions about the technical support.These factors are abstract constructs that can be measured in a survey using multi-item scales. The following items define each construct:  

- X18 Delivery Speed 
- X9 Complain resolution
- X16 Order and Billing,
to express “Customer service” 

- X11Product line
- X6 Product quality
- X13 Competing pricing,
to express “Product value”

- X12 Salesforce image
- X7 E-commerce
- X10 Advertising, 
to express “Marketing” 

- X8 Technical support 
- X14 Warranty and claims,
to express “Technical support”

##### 1. Data uploading, etc. 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(foreign)
data <- read.spss("../Data/HBAT.sav", to.data.frame=TRUE)
```

```{r}
# install.packages("lavaan", dependencies=TRUE)
library(lavaan)
```

```{r eval=FALSE}
variable.names(data)
# VariableLabels <- unname(attr(data, "variable.labels"))
# data.label.table <- attr(sav, "label.table") # if you load it with read_sav()
summary(data)

library(DataExplorer)
plot_histogram(data[, c(7:19)])

```

##### 2. Check if Exploratory factor analysis (EFA) applies

```{r eval=FALSE}
# Look at their the coefficient of linear correlation
cormatrix <- cor(data[, c(7:19)])
round(cormatrix, 2)
# Observe that most of the variables have a high correlation (>0.40 as a rule-of-thumb) with at least one of the others. x15 does not have decent correlation with any of the others; the analyst may decide to delete x15. If the analyst does not discover the problem here, it will be evident a few steps later. 

# In addition check, partial correlations (pairwise correlations are adjusted for the effects of all other variables)
 library(ppcor)

 pcor(data[, c(7:19)])
# If there are factors present, then high bivariate correlations become very low partial correlations

```

```{r eval=FALSE}
# Bartlett Sphericity Test. The null hypothesis is that the data dimension reduction is not possible. 
# It tests the degree that the corr matrix deviates from an identity matrix.
# If p-value < 0.05, we reject the null hypothesis and apply FA. This test is sensitive to N.
library(psych)
print(cortest.bartlett(cor(data[, c(7:19)]), nrow(data[, c(7:19)])))
```

```{r eval=FALSE}
# Kaiser-Meyer-Oklin Test (KMO). MSA overall should be .60 or higher to proceed with factor analysis
# The statistic is a measure of the proportion of variance among variables that might be common variance
library(psych)
KMO(data[, c(7:19)])
```
&nbsp;
&nbsp;



##### 3. Apply EFA with Common Factor Analysis
```{r eval=FALSE}
# Extracting as many factors as variables and check eigenvalues 

library(nFactors)

ev <- eigen(cor(data[, c(7:19)])) # calculate eigenvalues

ev$values # the first 4 factors have an eigenvalue >1

plot(ev$values, type="line") # the screeplot suggests 3 factors

# we go for 4 factors and see later if necessary to reduce this number

      
fit1 = factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x15 + x16 + x17 + x18, factors = 4, data = data, lower=0.10, rotation = "varimax")
# factanal() function can analyze raw data or a correlation matrix or covariance matrix; 
# factanal() applies Maximum Likelihood by default. 
print(fit1, sort=TRUE, cutoff=0.2)

# OUTPUT

  # Uniquenesses: 
  #   x6    x7    x8    x9   x10   x11   x12   x13   x14   x15   x16   x17   x18 
  # 0.623 0.305 0.285 0.183 0.663 0.100 0.100 0.595 0.100 0.987 0.358 0.100 0.100 
        
  # Uniqueness is the variance that is 'unique' to the variable and not shared with other variables.           
  # It is equal to 1 – communality (variance that is shared with other variables)
  

  # Loadings:
  #    Factor1 Factor2 Factor3 Factor4
  # x9   0.879                         
  # x16  0.784                         
  # x18  0.940                         
  # x6           0.609                 
  # x11  0.500   0.814                 
  # x13         -0.561   0.219         
  # x17  0.553  -0.750   0.204         
  # x7                   0.826         
  # x10                  0.530         
  # x12                  0.929         
  # x8                           0.838 
  # x14                          0.931 
  # x15               
  
  #               Factor1 Factor2 Factor3 Factor4
  # SS loadings      2.910   2.030   1.990   1.657
  # Proportion Var   0.224   0.156   0.153   0.127
  # Cumulative Var   0.224   0.380   0.533   0.661
  
  # Test of the hypothesis that 4 factors are sufficient.
  # The chi-square statistic (comparison of the actual corr matrix with
  # the fitted matrix) is 162.89 on 32 degrees of freedom.
  # The p-value is 1.83e-19 

# GUIDELINES FOR OUTPUT INTERPRETATION
  # To ease interpretation, very low loadings (<.1, <.2) are not displayed
  # According to the guidelines:
  # High loadings (>0.6 or > 0.7) are expected for the items 
  # Items with high cross-loadings should be removed 
  # communalities should be >50 to retain the item in the analysis
  # factor loadings should be high to retain the item in the analysis
  # and eigenvalues should be >=1 for factor selected
  # cummulative variance explained >0.60

  # The chi-square test is highly significant (p-value is 1.83e-19) 
  # meaning poor fit. But this test is not always reliable because it is highly 
  # influenced by the sample size analyzed. 
  # !!! NOTE THIS IS A POOR FIT !!! because H0 is there is low deviance between actual and fitted matrix

  # Loadings
  # x11 and x17 load high simultaneously on factor 1 and factor 2
  # This phenomenon is called  "cross-loading"
  # It means these items do not measure a single construct 
  # They are candidates for deletion 

  # x15 which does not correlate with any of the four factors
  # Does x15 represent a factor for which we do not have sufficient 
  # observable variables (measurements)?
  
  # Near the bottom of the output, we also see a test
  # of the hypothesis that 4 factors are sufficient.
  # The chi-square fit statistic is very small, indicating
  # the the hypothesis of perfect model fit is rejected. 

  # Uniqueness for each item are 1-communality
  1 - apply(fit1$loadings^2,1,sum)
  fit1$uniquenesses
  # Communalities
  apply(fit1$loadings^2,1,sum)
  # as one can observe, some items like x15 have very low communality
        

# Refining EFA without x15 and x17 (x11 could also be a candidate)
fit2 = factanal(~ x6 + x7 + x8 + x9 + x10 + x11 + x12 + x13 + x14 + x16 + x18, factors = 4, data = data, lower=0.1, rotation = "varimax")
print(fit2, sort=TRUE, cutoff=0.3)

# Uniquenesses:
#   x6    x7    x8    x9   x10   x11   x12   x13   x14   x16   x18 
# 0.635 0.305 0.285 0.163 0.668 0.100 0.100 0.599 0.100 0.342 0.100 

# Loadings:
#    Factor1 Factor2 Factor3 Factor4
# x9   0.895                         
# x16  0.796                         
# x18  0.918                         
# x7           0.827                 
# x10          0.537                 
# x12          0.928                 
# x8                   0.838         
# x14                  0.932         
# x6                           0.598 
# x11  0.519                   0.786 
# x13                         -0.553 

#                Factor1 Factor2 Factor3 Factor4
# SS loadings      2.613   1.962   1.645   1.391
# Proportion Var   0.238   0.178   0.150   0.126
# Cumulative Var   0.238   0.416   0.565   0.692

# Test of the hypothesis that 4 factors are sufficient.
# The chi square statistic is 26.7 on 17 degrees of freedom.
# The p-value is 0.0626 


# Now, the p-value associated with the chi-square statistic 
# 0.0626. This result is much promising. 
# The analysis can still be refined by excluding x11. Some researchers
# make a compromise and keep x11 in the model if the scale for the corresponding
# item was validated previously in the literature. 

# Last step: Naming the factors 
# x9, x16, x18 load high on Factor 1 ; we call it Customer service
# x7, x10, x12 load high on Factor 2; we call it Marketing
# x8 and x14 load high on Factor 3; we call it Technical support
# x6, x11, x13 load high on Factor 4;  we call it Product value

# OBS. Item x13 was negatively formulated in the questionnaire. 
# It should be reversed if the analysis if used further in the CFA and SEM.
```

Next, we set up a confirmatory factor analysis to confirm the measurement model (CFA). 
Finally, given the measurement model has been examined and validated in the CFA analysis, we set up a SEM model, to test the structural relationships between the four constructs identified and the customers´ likelihood to continue doing business with the company (X19-Satisfaction, X20-Likelihood of recommendation and X21-Likelihood of future purchase).

&nbsp;
&nbsp;


##### 4. Confirmatory Factor Analyis (CFA)
```{r eval=FALSE}
CFA.model <- 'CS =~ x18 + x9 + x16
             PV =~ x11 + x6 + x13
             MK =~ x12 + x7 + x10
             TS =~ x8 + x14
    # Correlations between exogeneous constructs are optional because
    # by default, all exogenous latent variables in a CFA model are allowed to correlate
        CS ~~ PV
        CS ~~ MK
        CS ~~ TS
        PV ~~ MK
        PV ~~ TS
        MK ~~ TS'

# fit the model
fit <- cfa(CFA.model, data = data)
# display summary output
summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE)

# NOTE: we get "lavaan WARNING: some estimated ov variances are negative". 
# This is called in the literature "Heywood case". Heywood cases or negative variance estimates, are a common occurrence in factor analysis and latent variable structural equation models.
# There are several potential causes (https://journals.sagepub.com/doi/10.1177/0049124112442138). Here,eliminating the problematic item x11, will solve the problem.  
# ´ov´ means observed variable

# Before doing that, I ask for the modification indices as a last check
modificationindices(fit, sort = T, minimum.value = 10, op = "~~")
# MI reveal that x11 is correlated with x16 and x18; it means that x11 has substantial cross-loading on two factors (as in EFA). Cross-loading goes against one of the principles of unidimensionality in SEM. We delete x11 from the analysis and re-run CFA. 

```


```{r}
# CFA model after deleting x11
set.seed(1234)
CFA.model <- 'CS =~ x18 + x9 + x16
             PV =~ x6 + x13
             MK =~ x12 + x7 + x10
             TS =~ x8 + x14'
# fit the model
fit <- cfa(CFA.model, data = data)

# display summary output
summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE)

inspect(fit, what="resid") # new line! I think this is the residual matrix. Look for large residuals.

# output not displayed; see in your console

# 1.) Examine the model fit; guidelines for a good model: 
#     CFI >.90, TLI>.90, RMSEA< 0.08, SRMR <.0.08. 
#     Some sources require CFI >.95, TLI>.95, RMSEA< 0.05, SRMR <.0.05. 


# 2). Examine the loadings significance, size and sign 
#     It is desirable to have high and significant loading - 
#     reflecting items convergent validity. In the output 
#     standardized loadings are in the last column "Std.all""

    # In one factor 2, competitive pricing (x13) and product quality (x6)
    # have opposite signs. It means that the product quality and competitive 
    # pricing vary together, but move in direction opposite to each other. 
    # Perceptions are more positive whether product quality increases
    # or price decreases. This trade-off leads to naming the factor "product value". 
    # When variables have different signs, we need to be careful to reverse one when creating
    # summated scales or using further in SEM analysis. 
    # given our variable is using a 0-10 scale , to reverse it we take:
    data$x13r = 10-data$x13


    # and re-run
    CFA.model <- '
    # Measurement model
        CS =~ x18 + x9 + x16
        PV =~ x6 + x13r
        MK =~ x12 + x7 + x10
        TS =~ x8 + x14'
    
    fit <- cfa(CFA.model, data = data)
    summary(fit, fit.measures=TRUE, standardized = TRUE, modindices = FALSE)
    
    
# 3). Examine VALIDITY and RELIABILITY of the factors - 
#   Reliability = degree of consistency between multiple measurements of a variable 
#   install.packages("semTools")
    library(semTools)
    semTools::reliability (fit) # one can conclude that all factors display good reliability
    # alpha =  coefficient alpha (Cronbach, 1951) - should be > than 0.5 or 0.6 
    # omega = similar to composite reliability index (CR) (Fornell & Larcker (1981) - should be > 0.7
    # avevar = average variance extracted (AVE) (Fornell & Larcker (1981))  - should be > than 0.5.


    
# 4). Examine DISCRIMINANT VALIDITY of the factors
    # the latent variables can be thought of representing two distinct constructs.
    semTools::discriminantValidity(fit, merge=TRUE)
    # https://rdrr.io/cran/semTools/man/discriminantValidity.html
    # Output: 
    # The first set are factor correlation estimates and their confidence intervals.
    # Are these correlations sufficiently low to claim discriminant validity of the four constructs? 
    # Based on Fornell & Larcker (1981), the square root of each construct´s AVE should have a greater value 
    # than the inter-constructs corelations (or AVE > corr^2). Let us check that:
    reliability_out = reliability (fit)
    AVEs = reliability_out[5,]
    sqrtAVEs = sqrt(AVEs)
    sqrtAVEs
    # Comparing the inter-constructs correlations (see "est"" column in the output of        
    # discriminantValidity(fit, merge=TRUE)) with the sqrtAVEs, conclude that
    # the four constructs display significant discriminant validity. 
```

```{r, echo=FALSE}
library(lavaanPlot)
lavaanPlot(model = fit, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, covs=TRUE, stand=TRUE, sig=.05, stars="covs")  
```

```{r} 
# new line!
inspect(fit, what="resid")
```



##### 5. SEM
```{r} 
# Dependent variable: x19-Customer Satisfaction. Build a model to explain x19;
# NOTE: Three variables were not included in the CFA (x11, x15, x17). 
#       Reason: These variables did not load high on any of the main constructs
#       If they are important in theory, they can be treated as separate explanatory variables in SEM


SEM.model1 <- '
# Measurement model
        CS =~ x18 + x9 + x16
        PV =~ x6 + x13r
        MK =~ x12 + x7 + x10
        TS =~ x8 + x14
# Structural model
        x19 ~ CS + PV + MK + TS'

# fit the model
fitSEM1 <- sem(SEM.model1, data=data, se ="robust", estimator = "ML") 
summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE)

# The message "lavaan WARNING: some estimated ov variances are negative" shows up. 
# In this case, the problematic items are x12 and x14. It reflects that we would need more
# items per construct to run a good model. 
# We set se="robust" to produce robust standard errors; 
# setting se="boot" or se="bootstrap" will produce bootstrap standard errors. 
# Check the fit indexes for SEM model as done in CFA 
# fitmeasures(fit) # alternative summary
 
# Next, check the structural coefficients in summary(). Output partially reproduced below: 
# Regressions:
  #                 Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all
  #x19 ~                                                                 
 #   CS                0.787    0.116    6.758    0.000    0.534    0.450
 #   PV                0.663    0.110    6.022    0.000    0.746    0.629
 #   MK                0.518    0.078    6.671    0.000    0.595    0.502
 #   TS               -0.039    0.045   -0.870    0.384   -0.042   -0.035

 # Concl.: Customers perceptions about CS, PV and MK are positively and significantly correlated with satisfaction. 
#          TS (Technical Service) perceptions is not significantly related to customer satisfaction. 
 
# check modification indices if relevant
  summary(fitSEM1, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE)
  modificationindices(fitSEM1, sort = T, minimum.value = 10, op = "~~")
# no suggestion for improvement
```

```{r}
# The bootstrap model parameters are available with:
# PAR.boot <- bootstrapLavaan(fitSEM1, R=10, type="ordinary",FUN="coef")
# T.boot <- bootstrapLavaan(fitSEM1, R=10, type="bollen.stine",FUN=fitMeasures, fit.measures="chisq")

```

```{r, echo=FALSE}
#### Plotting the model
library(lavaanPlot)
labels <- list(x19 = "SATISFACTION")
lavaanPlot(model = fitSEM1, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, covs=TRUE, stand=TRUE, sig=.05, stars="regress", labels = labels)   

#or
library(semPlot)
semPaths(fitSEM1, "std", intercepts = FALSE, style="lisrel", layout="tree2")
```



##### 6. An extended SEM model
```{r}
# SEM correcting the model (notice we have some negative variances in the previous model for x12 and x14; we start with eliminating the problematic item x14). 
# Plus extending the model consistent with the theory: Sem.model2 proposed x19 (Satisfaction) as mediator between the four latent constructs 
# and Likelihood of future purchase (x21)

SEM.model2 <- '
# Measurement model
             CS =~ x18 + x9 + x16
             PV =~ x6 + x13r
             MK =~ x7 + x10
             TS =~ x8 
# Structural model
             x19 ~ CS + PV + MK + TS
             x21 ~ x19'

# fit the model
fitSEM2 <- sem(SEM.model2, data=data, se="robust") 
summary(fitSEM2, fit.measures=TRUE)


summary(fitSEM2, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE)
modificationindices(fitSEM2, sort = T, minimum.value = 10, op = "~~")
# model has a good fit
```

```{r pressure, echo=FALSE}
# plot
library(lavaanPlot)
labels <- list(x19 = "SATISFACTION", x21 = "FUTURE PURCHASE")
lavaanPlot(model = fitSEM2, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, covs=TRUE, stand=TRUE, sig=.05, stars="regress", labels = labels)


 # - summarizing the findings 
 # - are the all structural paths in the sem model significant? 
 # - which is the most important determinant of customer satisfaction? (check std. path coefficients and conclude)
 # - does satisfaction act as a significant mediator? (check the sig. of mediating patterns and conclude)
 # - how much variance in x21 (Likelihood of future purchase) the model explains? (check R^2 associated)
```



```{r pressure, echo=FALSE}
# Introducing categorical variables in the model
# If binary exogenous covariate, we recode it as a dummy (0/1) variable
# as an example, recode X4 (Region) as 0/1 and introduce it in the model

# Hypothesis 1: Region to which customer belongs has an effect on future purchase
data$x4d <- ifelse(data$x4 == "USA/North America", 1, 0)
SEM.model3 <- '
# Measurement model
             CS =~ x18 + x9 + x16
             PV =~ x6 + x13r
             MK =~ x7 + x10
             TS =~ x8 
# Structural model
             x19 ~ CS + PV + MK + TS
             x21 ~ x19 + x4d'

# fit the model
fitSEM3 <- sem(SEM.model3, data=data, estimator = "ML") 
summary(fitSEM3, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE)
# the fit is not very good; my hypothesis does not hold
modificationindices(fitSEM3, minimum.value = 3.84) 
# MI suggest a possible effect of Region on perceived value (PV)


# Hypothesis 2: Region to which customer belongs has an effect on perceived value
SEM.model3 <- '
# Measurement model
             CS =~ x18 + x9 + x16
             PV =~ x6 + x13r
             MK =~ x7 + x10
             TS =~ x8 
# Structural model
             x19 ~ CS + PV + MK + TS
             x21 ~ x19 + x4d
             PV ~ x4d'

# fit the model
fitSEM3 <- sem(SEM.model3, data=data) 
summary(fitSEM3, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE)
# model improved significantly;
# coefficient of x4d is significant and positive; 
# thus, we found evidence for our second hypothesis;
# to interpret the positive coefficient, given that "USA/North America" is "1",
# it means the customers from this region score significantly 
# more on PV than customers from other regions

```

```{r pressure, echo=FALSE}
# If we have an exogenous nominal categorical variable with K > 2 levels, we
# replace it by a set of K-1 dummy variables
# as an example recode X1 as k-1 dummy ('Less than 1 year' as ref)
prop.table(table(data$x1))
data$x1_1 <- ifelse(data$x1 == "1 to 5 years", 1, 0)
data$x1_2 <- ifelse(data$x1 == "Over 5 years", 1, 0)


# H1: Customers who have a longer history with the company, such as longstanding
# customers, will have a more positive evaluation of product and will be 
# more satisfied with customer service than recently acquired customers. 
SEM.model3 <- '
# Measurement model
             CS =~ x18 + x9 + x16
             PV =~ x6 + x13r
             MK =~ x7 + x10
             TS =~ x8 
# Structural model
             x19 ~ CS + PV + MK + TS
             x21 ~ x19 
             PV ~ x1_1 + x1_2
             CS ~ x1_1 + x1_2'

# fit the model
fitSEM3 <- sem(SEM.model3, data=data) 
summary(fitSEM3, fit.measures=TRUE, standardized = TRUE, rsquare=TRUE, modindices=TRUE)
modificationindices(fitSEM3, minimum.value = 3.84)
#check output and interpret
```


```{r pressure, echo=FALSE}
# final note, if you have Likert scale for the measurement items, and you want to
# declare them as `ordered', do this before you run the analysis
Data[,c("item1","item2","item3","item4")] <-lapply(Data[,c("item1","item2","item3","item4")], ordered)
```





